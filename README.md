

# ğŸ§  NeuraFusion - Multimodal AI Assistant

<div align="center">

![NeuraFusion Logo](https://img.shields.io/badge/NeuraFusion-AI%20Assistant-blue?style=for-the-badge&logo=python)

[![Python](https://img.shields.io/badge/Python-3.10+-green?style=flat-square&logo=python)](https://www.python.org/)
[![Gradio](https://img.shields.io/badge/Gradio-5.x-orange?style=flat-square&logo=gradio)](https://gradio.app/)
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Transformers-yellow?style=flat-square&logo=huggingface)](https://huggingface.co/)
[![License](https://img.shields.io/badge/License-MIT-blue?style=flat-square)](LICENSE)

[![Demo](https://img.shields.io/badge/ğŸ§ª-Try%20Demo-red?style=for-the-badge)](https://huggingface.co/spaces/mohitbansal25082006/NeuraFusion)
[![GitHub](https://img.shields.io/badge/ğŸ“-View%20on%20GitHub-black?style=for-the-badge)](https://github.com/mohitbansal25082006/NeuraFusion)

*A powerful multimodal AI assistant that combines text, image, and audio processing with customizable personalities*

</div>

## ğŸ“‹ Table of Contents

- [ğŸ¯ Project Overview](#-project-overview)
- [âœ¨ Features](#-features)
- [ğŸ› ï¸ Technology Stack](#ï¸-technology-stack)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸš€ Installation](#-installation)
- [ğŸ’» Usage Guide](#-usage-guide)
- [ğŸŒ Deployment Options](#-deployment-options)
- [ğŸ’° Cost Analysis](#-cost-analysis)
- [ğŸ“ Learning Outcomes](#-learning-outcomes)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“œ License](#-license)
- [ğŸ™ Acknowledgments](#-acknowledgments)

---

## ğŸ¯ Project Overview

**NeuraFusion** is a comprehensive multimodal AI assistant that processes text, images, and audio through an intuitive web interface. Built with free-tier Hugging Face models and optional OpenAI integration, it offers a complete AI experience without expensive infrastructure requirements.

### Key Features

- ğŸ§  **Multimodal Processing**: Understand and process text, images, and audio
- ğŸ­ **5 AI Personalities**: Choose from Mentor, Friend, Analyst, Professional, or Creative modes
- ğŸ” **Visual Analysis**: Advanced image understanding with attention heatmaps
- ğŸ¤ **Voice Interaction**: Speech-to-text and text-to-speech capabilities
- ğŸ’¾ **Conversation Memory**: Context-aware responses with conversation history
- ğŸ“Š **Analytics Dashboard**: Track usage patterns and preferences
- ğŸ’¼ **Export Options**: Save conversations in multiple formats (JSON, Text, Markdown, CSV)

---

## âœ¨ Features

### ğŸ§  Multimodal Capabilities

| Feature | Description | Models Used |
|---------|-------------|-------------|
| **Text Processing** | Advanced text understanding and generation | Flan-T5, GPT-4o (optional) |
| **Image Analysis** | Visual question answering and captioning | BLIP-2, GPT-4o Vision (optional) |
| **Audio Processing** | Speech recognition and synthesis | Whisper, gTTS |
| **Multimodal Fusion** | Combined reasoning across all modalities | Custom fusion engine |

### ğŸ­ Personality System

1. **ğŸ“ Mentor Mode**
   - Patient and educational approach
   - Detailed explanations with examples
   - Perfect for learning new concepts

2. **ğŸ˜Š Friend Mode** (Default)
   - Casual and conversational tone
   - Warm and empathetic responses
   - Great for everyday chat

3. **ğŸ“Š Analyst Mode**
   - Data-driven and logical responses
   - Structured analysis approach
   - Ideal for research and analysis

4. **ğŸ’¼ Professional Mode**
   - Formal business communication style
   - Concise and actionable responses
   - Best for work-related queries

5. **ğŸ¨ Creative Mode**
   - Imaginative and expressive responses
   - Unique perspectives and ideas
   - Perfect for brainstorming

### ğŸ” Visual Analysis Features

- **Attention Heatmaps**: Visualize where the AI "looks" in images
- **Color Distribution Analysis**: RGB channel breakdowns and histograms
- **Feature Visualization**: Extract and display image characteristics
- **Object Recognition**: Identify and describe objects in images

### ğŸ’¾ Export & Memory System

- **4 Export Formats**: JSON, Text, Markdown, CSV
- **Conversation History**: Complete session tracking with timestamps
- **Context Management**: Intelligent conversation memory
- **Session Analytics**: Usage statistics and preferences

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Parameters | Cost |
|-----------|-----------|------------|------|
| **Text Generation** | Flan-T5 Base | 250M | Free |
| **Vision** | BLIP-2 OPT | 2.7B | Free |
| **Speech-to-Text** | Whisper Base | 74M | Free |
| **Text-to-Speech** | gTTS | - | Free |
| **Premium Text** | GPT-4o (optional) | 1.76T | $0.0025/1K tokens |
| **Premium Vision** | GPT-4o Vision | - | $0.01/1K tokens |
| **UI Framework** | Gradio 5.x | - | Free |
| **Visualization** | Matplotlib + Seaborn | - | Free |
| **Memory** | LangChain | - | Free |

---

## ğŸ“ Project Structure

```
F:/NeuraFusion/
â”‚
â”œâ”€â”€ ğŸ“ venv/                          # Anaconda virtual environment
â”‚
â”œâ”€â”€ ğŸ“ models_cache/                  # Downloaded model weights (auto-created)
â”‚
â”œâ”€â”€ ğŸ“ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text_processor.py             # Text processing utilities
â”‚   â”œâ”€â”€ image_processor.py            # Image analysis functions
â”‚   â”œâ”€â”€ audio_processor.py            # Audio transcription & TTS
â”‚   â”œâ”€â”€ fusion_engine.py              # Multimodal fusion logic
â”‚   â”œâ”€â”€ memory_manager.py             # Conversation context
â”‚   â””â”€â”€ visualization.py              # Attention heatmaps
â”‚
â”œâ”€â”€ ğŸ“ assets/
â”‚   â”œâ”€â”€ icons/                        # UI icons
â”‚   â”œâ”€â”€ samples/                      # Sample test files
â”‚   â”‚   â”œâ”€â”€ sample_image.jpg
â”‚   â”‚   â””â”€â”€ sample_audio.mp3
â”‚   â””â”€â”€ styles.css                    # Custom CSS for Gradio
â”‚
â”œâ”€â”€ ğŸ“ config/
â”‚   â”œâ”€â”€ model_configs.json            # Model paths and settings
â”‚   â””â”€â”€ personalities.json            # Personality presets
â”‚
â”œâ”€â”€ app.py                            # Main Gradio application
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ .env.example                      # Environment variables template
â”œâ”€â”€ .gitignore                        # Git ignore file
â”œâ”€â”€ README.md                         # Project documentation
â””â”€â”€ test_models.py                    # Script to test model loading
```

---

## ğŸš€ Installation

### Prerequisites

- **Python 3.10+**
- **Anaconda/Miniconda** (recommended)
- **Git**
- **8GB+ RAM** (16GB recommended)
- **10GB+ free disk space**

### Setup Instructions

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/neurafusion.git
   cd neurafusion
   ```

2. **Create and activate virtual environment**
   ```bash
   conda create -n neurafusion python=3.10
   conda activate neurafusion
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env file with your API keys (optional)
   ```

5. **Run the application**
   ```bash
   python app.py
   ```

6. **Access the application**
   - Open your browser and go to `http://127.0.0.1:7860`

---

## ğŸ’» Usage Guide

### ğŸ’¬ Smart Chat Tab

**Best For:**
- General conversation
- Q&A
- Learning and explanations
- Brainstorming

**Tips:**
- Switch personalities to match your needs
- Mentor for learning, Analyst for research
- Friend for casual chat, Professional for work

### ğŸ¤ Voice Assistant Tab

**Best For:**
- Hands-free interaction
- Language practice
- Accessibility
- Quick questions

**Tips:**
- Speak clearly in quiet environment
- Keep recordings under 30 seconds
- Use microphone button for live recording
- Upload longer audio files if needed

### ğŸ–¼ï¸ Vision Analysis Tab

**Best For:**
- Image description
- Visual question answering
- Color analysis
- Object identification

**Features to Try:**
- Ask specific questions about images
- Enable attention maps to see AI focus
- Use color analysis for design insights
- Try different personalities for varied descriptions

### ğŸ”— Complete Fusion Tab

**Best For:**
- Complex queries with multiple inputs
- Research and analysis
- Content creation
- Comprehensive understanding

**Examples:**
- Upload vacation photo + ask "Where was this taken?" via voice
- Show recipe image + ask "How can I make this healthier?" in text
- Record audio question + upload relevant image for context

### ğŸ“Š Analytics Tab

**Best For:**
- Tracking usage patterns
- Understanding your habits
- System monitoring
- Performance insights

### ğŸ’¾ Memory & Export Tab

**Best For:**
- Saving conversations
- Reviewing history
- Sharing insights
- Data analysis

---

## ğŸŒ Deployment Options

### Option 1: Local Development

- Runs on your PC
- Access via: http://127.0.0.1:7860
- Private and secure

### Option 2: Hugging Face Spaces (Free Hosting)

1. Create a Hugging Face account at https://huggingface.co/join
2. Create a new Space with the Gradio SDK
3. Upload all project files
4. Add your environment variables
5. Deploy and get a public URL

### Option 3: Share Temporarily

- In `app.py`, change: `share=True`
- Get a temporary public link
- Valid for 72 hours
- No setup needed!

---

## ğŸ’° Cost Analysis

| Component | Cost |
|-----------|------|
| Hugging Face Models | $0 (free) |
| HF Spaces Hosting | $0 (free tier) |
| gTTS (Text-to-Speech) | $0 (free) |
| OpenAI GPT-4o API (optional) | $5-20 (pay-as-you-go) |
| ElevenLabs TTS (optional) | $0-5 (free tier: 10k chars/month) |
| **Total (Free Version)** | **$0** |
| **Total (Premium Version)** | **$5-25** |

---

## ğŸ“ Learning Outcomes

By completing this project, you'll master:

âœ… **Hugging Face Ecosystem** - Transformers, Spaces, Models Hub  
âœ… **Multimodal AI** - Text, Vision, Audio processing  
âœ… **Gradio Framework** - Interactive ML demos  
âœ… **LangChain** - AI orchestration and memory  
âœ… **API Integration** - OpenAI, Hugging Face, ElevenLabs  
âœ… **Deployment** - Cloud hosting, Git workflows  
âœ… **Python Best Practices** - Virtual environments, project structure  

---

## ğŸ¤ Contributing

We welcome contributions to NeuraFusion! Here's how you can help:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Development Guidelines

- Follow the existing code style and conventions
- Write clear, descriptive commit messages
- Add tests for new features
- Update documentation as needed

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

### AI Models
- **Flan-T5**: Google Research
- **BLIP-2**: Salesforce Research
- **Whisper**: OpenAI
- **GPT-4o**: OpenAI

### Frameworks
- **Gradio**: Hugging Face Team
- **Transformers**: Hugging Face
- **LangChain**: LangChain Team

### Tools
- **Python**: PSF
- **PyTorch**: Meta AI
- **Matplotlib**: NumFOCUS

---

<div align="center">

**Made with â¤ï¸ and ğŸ§  | NeuraFusion V3.0**

[![GitHub stars](https://img.shields.io/github/stars/yourusername/neurafusion?style=social)](https://github.com/mohitbansal25082006/neurafusion)
[![GitHub forks](https://img.shields.io/github/forks/yourusername/neurafusion?style=social)](https://github.com/mohitbansal25082006/neurafusion)

</div>
